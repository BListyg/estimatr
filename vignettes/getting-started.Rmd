---
title: 'Getting started using estimatr'
author: "Luke Sonnet"
output:
  html_document:
    df_print: paged
link-citations: yes
bibliography: estimatr.bib
vignette: |
  %\VignetteIndexEntry{Getting started using estimatr} 
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8]{inputenc}
---

\newcommand{\X}{\mathbf{X}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Gb}{\mathbf{G}}
\newcommand{\XtXinv}{(\X^{\top}\X)^{-1}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\V}{\mathbb{V}}

```{r, echo = FALSE}
set.seed(42)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(digits = 2)
```

**estimatr** is a package in R dedicated to providing [fast](articles/benchmarking-estimatr.html) estimators for social scientists. Estimators are statistical methods for estimating quantities of interest like treatment effects or regression parameters. Many of the estimators included with the R programming language or popular R packages are slow and have default settings that lead to statistically inappropriate estimates. Certain estimators that reflect cutting-edge advances in statistics are not yet implemented in R packages for convenient use. **estimatr** is designed to solve these problems and provide estimators tuned for design-based inference.

The most up-to-date version of this vignette can be found on the [DeclareDesign website here](http://estimatr.declaredesign.org/articles/getting-started.html).

# Estimators

The current estimators we provide are:

* [`lm_robust`](#lm_robust) - for fitting linear models with heteroskedasticity/cluster robust standard errors
* [`lm_lin`](#lm_lin) - a wrapper for `lm_robust()` to simplify interacting centered pre-treatment covariates with a treatment variable
* [`difference_in_means`](#difference_in_means) - for estimating differences in means with appropriate standard errors for simple, cluster randomized, block randomized, matched-pair designs and more
* [`horvitz_thompson`](#horvitz_thompson) - for estimating average treatment effects taking into consideration treatment probabilities or sampling probabilities for simple and cluster randomized designs

To demonstrate basic usage of each of the estimators, I will first create some sample data.

```{r echo=TRUE, results="hide"}
library(estimatr)

# Example dataset to be used throughout built using fabricatr and randomizr
library(fabricatr)
library(randomizr)
dat <- fabricate(
  N = 100,                        # sample size
  x = runif(N, 0, 1),             # pre-treatment covariate
  y0 = rnorm(N, mean = x),        # control potential outcome
  y1 = y0 + 0.35,                 # treatment potential outcome
  z = randomizr::complete_ra(N),  # complete random assignment to treatment
  y = ifelse(z, y1, y0),          # observed outcome

  # We will also consider clustered data
  clust = sample(rep(letters[1:20], each = 5)),
  z_clust = cluster_ra(clust),
  y_clust = ifelse(z_clust, y1, y0)
)

head(dat)
```
```{r echo=FALSE}
knitr::kable(head(dat))
```

## `lm_robust`

The `estimatr` package provides `lm_robust()` to quickly fit linear models with the most common variance estimators and degrees of freedom corrections used in social science. You can easily estimate heteroskedastic standard errors, clustered standard errors, and classical standard errors.

Usage largely mimics `lm()`, although it defaults to using Eicker-Huber-White robust standard errors, specifically "HC2" standard errors. More about the exact specifications used can be found in the [technical notes](articles/technical-notes.html#lm_robust-notes) and more about the estimator can be found on its reference page: `lm_robust()`.

```{r, lm_robust, results="hide"}
res <- lm_robust(y ~ z + x, data = dat)
summary(res)
tidy(res) # summary(res) is very similar
```
```{r, echo=FALSE}
knitr::kable(tidy(res))
```

It is straightforward to do cluster-robust inference, by passing the name of your cluster variable to the `clusters =` argument. Note that `lm_robust()` is much quicker if your cluster variable is a factor!

```{r, echo=TRUE, results="hide"}
res_cl <- lm_robust(
  y_clust ~ z_clust + x,
  data = dat,
  clusters = clust
)
tidy(res_cl)
```
```{r echo=FALSE}
knitr::kable(tidy(res_cl))
```

The default variance estimator with clusters is dubbed 'CR2' because it is analogous to 'HC2' for the clustered case, and utilizes recent advances proposed by @pustejovskytipton2016 to correct hypotheses tests for small samples and work with commonly specified fixed effects and weights.

Researchers can also replicate Stata's standard errors by using the `se_type =` argument both with and without clusters:
```{r echo=TRUE, results="hide"}
res_stata <- lm_robust(
  y_clust ~ z_clust + x,
  data = dat,
  clusters = clust,
  se_type = "stata"
)
tidy(res_stata)
```
```{r echo=FALSE}
knitr::kable(tidy(res_stata))
```

## `lm_lin`

Adjusting for pre-treatment covariates when using regression to estimate treatment effects is common practice across scientific disciplines. However, @freedman2008 demonstrated that pre-treatment covariate adjustment biases estimates of average treatment effects. In response, @lin2013 proposed an alternative estimator that would reduce this bias and improve precision. @lin2013 proposes centering all pre-treatment covariates, interacting them with the treatment variable, and regressing the outcome on the treatment, the centered pre-treatment covariates, and all of the interaction terms. This can require a bit of pre-processing.

To facilitate this, we provide a wrapper that can does this whole process for you in an estimator we dub `lm_lin()`. This function is a wrapper for `lm_robust()`, and all arguments that work for `lm_robust()` work here. The only difference is in the second argument `covariates`, where one specifies a right-sided formula with all of your pre-treatment covariates. Below is an example, and more can be seen on the function reference page [`lm_lin`](#lm_lin) and some formal notation can be seen in the [technical notes](#lm_lin-notes).

```{r echo=TRUE, results="hide"}
res_lin <- lm_lin(
  y ~ z,
  covariates = ~ x,
  data = dat
)
tidy(res_lin)
```
```{r echo=FALSE}
knitr::kable(tidy(res_lin))
```

## `difference_in_means`

While estimating differences in means may seem straightforward, we provide a function that appropriately adjusts estimates for experimental design, whether it be blocking, clustering, matched pairs or a more complicated design. Usage is similar to usage in regression functions and here are a few short examples; more can be seen on the function reference page: `difference_in_means()`.

Technical details are available [here](articles/technical-notes.html#lm_robust-notes).

```{r echo=TRUE, results="hide"}
# Simple version
res_dim <- difference_in_means(
  y ~ z,
  data = dat
)
tidy(res_dim)
```
```{r echo=FALSE}
knitr::kable(tidy(res_dim))
```
```{r echo=TRUE, results="hide"}
# Clustered version
res_dim_cl <- difference_in_means(
  y_clust ~ z_clust,
  data = dat,
  clusters = clust
)
```
```{r echo=FALSE}
knitr::kable(tidy(res_dim_cl))
```

Users can see more examples 

## `horvitz_thompson`

Horvitz-Thompson estimators are useful when estimating treatment effects without bias when there is a clustered design with unequal size clusters or when the treatment assignment process is arbitrarily complex. Horvitz-Thompson estimators require information about the treatment (and control) probabilities for each unit, and the joint treatment and control probabilities of every pair of units. For designs that are not arbitrarily complex, passing your randomization scheme using `declare_ra()` from the [`randomizr`](randomizr.declaredesign.org) package is the easiest path forward. The technical details can be found [here](articles/technical-notes.html#horvitz_thompson-notes) and in the linked references.

```{r, results="hide"}
# Complete random assignment declaration
crs_decl <- declare_ra(
  N = nrow(dat),
  prob = 0.5,
  simple = FALSE
)

ht_comp <- horvitz_thompson(
  y ~ z,
  data = dat,
  declaration = crs_decl
)
tidy(ht_comp)
```
```{r, echo=FALSE}
knitr::kable(tidy(ht_comp))
```

We can also easily estimate treatment effects from a cluster randomized experiment. Letting `horvitz_thompson` know that the design is clustered means it uses a collapsed estimator for the variance described in @aronowmiddleton2013, which is preferred for clustered designs.

```{r, results = "hide"}
# Clustered random assignment declaration
crs_clust_decl <- declare_ra(
  N = nrow(dat),
  clusters = dat$clust,
  prob = 0.5,
  simple = FALSE
)

ht_clust <- horvitz_thompson(
  y_clust ~ z_clust,
  data = dat,
  declaration = crs_clust_decl
)
tidy(ht_clust)
```
```{r echo=FALSE}
knitr::kable(tidy(ht_clust))
```

You can also build the condition probability matrix (`condition_prob_mat = `) that `horvitz_thompson()` needs from a declaration from the [`randomizr`](randomizr.declaredesign.org) package---using `declaration_to_conditional_pr_mat()`---or from a matrix of permutations of the treatment vector---using `permutations_to_conditional_pr_mat()`. **This is largely intended for use by experienced users. Note, that if one passes a `condition_prob_mat` that indicates clustering, but not the clusters, then the collapsed estimator will not be used and the variance will be incorrect.**

```{r, results="hide"}
# arbitrary permutation matrix
possible_treats <- cbind(
  c(1, 1, 0, 1, 0, 0, 0, 1, 1, 0),
  c(0, 1, 1, 0, 1, 1, 0, 1, 0, 1),
  c(1, 0, 1, 1, 1, 1, 1, 0, 0, 0)
)
arb_pr_mat <- permutations_to_condition_pr_mat(possible_treats)

# Simulating a column to be realized treatment
dat <- data.frame(
  z = possible_treats[, sample(ncol(possible_treats), size = 1)],
  y = rnorm(nrow(possible_treats))
)

ht_arb <- horvitz_thompson(
  y ~ z,
  data = dat,
  condition_pr_mat = arb_pr_mat
)
tidy(ht_arb)
```
```{r echo=FALSE}
knitr::kable(tidy(ht_arb))
```

# References
