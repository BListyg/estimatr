<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Estimatr: fast estimators for social scientists • estimatr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">estimatr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="http://declaredesign.org">DeclareDesign</a>
    </li>
    <li>
      <a href="http://randomizr.declaredesign.org">randomizr</a>
    </li>
    <li>
      <a href="http://fabricatr.declaredesign.org">fabricatr</a>
    </li>
    <li>
      <a href="http://estimatr.declaredesign.org">estimatr</a>
    </li>
  </ul>
</li>
<li>
  <a href="http://discuss.declaredesign.org">Discussion Board</a>
</li>
<li>
  <a href="http://declaredesign.org">DeclareDesign home</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Estimatr: fast estimators for social scientists</h1>
                        <h4 class="author">Luke Sonnet</h4>
            
            <h4 class="date">2017-11-11</h4>
          </div>

    
    
<div class="contents">
<p><code>estimatr</code> is a package in r dedicated to providing fast estimators for social scientists. Many extant estimators in base r or popular packages are slow, have default settings that lead to statistically inappropriate estimates, or are unable to provide certain specifications.</p>
<p>This vignette will demonstrate the various estimators available in this package and provide justifications and citations for the default settings. The estimators currently available are:</p>
<ul>
<li>
<a href="#lm_robust"><code><a href="../reference/lm_robust.html">lm_robust()</a></code></a> - for fitting linear models with heteroskedasticity/cluster robust standard errors</li>
<li>
<a href="#lm_lin"><code><a href="../reference/lm_lin.html">lm_lin()</a></code></a> - a wrapper for <code><a href="../reference/lm_robust.html">lm_robust()</a></code> to simplify interacting centered pre-treatment covariates with a treatment variable</li>
<li>
<a href="#difference_in_means"><code><a href="../reference/difference_in_means.html">difference_in_means()</a></code></a> - for estimating differences in means with appropriate standard errors for simple, cluster randomized, block randomized, matched-pair designs and more</li>
<li>
<a href="#horvitz_thompson"><code><a href="../reference/horvitz_thompson.html">horvitz_thompson()</a></code></a> - for estimating average treatment effects taking into consideration treatment probabilities or sampling probabilities for simple and cluster randomized designs</li>
</ul>
<div id="hypothetical-experiment" class="section level1">
<h1 class="hasAnchor">
<a href="#hypothetical-experiment" class="anchor"></a>Hypothetical experiment</h1>
<p>TODO</p>
</div>
<div id="estimators" class="section level1">
<h1 class="hasAnchor">
<a href="#estimators" class="anchor"></a>Estimators</h1>
<div id="lm_robust" class="section level2">
<h2 class="hasAnchor">
<a href="#lm_robust" class="anchor"></a><code><a href="../reference/lm_robust.html">lm_robust()</a></code>
</h2>
<p>All of the following works with weights except for the clustered standard errors (todo: finish weights with clustered SEs and write up weights in below.)</p>
<div id="coefficient-estimates" class="section level3">
<h3 class="hasAnchor">
<a href="#coefficient-estimates" class="anchor"></a>Coefficient estimates</h3>
<p><span class="math display">\[
\hat{\beta} ={({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^{\top}{\mathbf{y}}\]</span></p>
</div>
<div id="variance" class="section level3">
<h3 class="hasAnchor">
<a href="#variance" class="anchor"></a>Variance</h3>
<p><strong>Definitions</strong></p>
<ul>
<li>
<span class="math inline">\({\mathbf{x}}_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\({\mathbf{X}}\)</span>.</li>
<li><span class="math inline">\(h_{ii} = {\mathbf{x}}_i{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{x}}^{\top}_i\)</span></li>
<li><span class="math inline">\(e_i = y_i - {\mathbf{x}}_i\hat{\beta}\)</span></li>
<li>
<span class="math inline">\(\mathrm{diag}[.]\)</span> is an operator that creates a diagonal matrix</li>
</ul>
<div id="heteroskedasticity-robust-variance" class="section level4">
<h4 class="hasAnchor">
<a href="#heteroskedasticity-robust-variance" class="anchor"></a>Heteroskedasticity-Robust Variance</h4>
<p><strong>HC2 (default)</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{HC2} =  {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^{\top}\mathrm{diag}\left[\frac{e_i^2}{1-h_{ii}}\right]{\mathbf{X}}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
<p><strong>HC0</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{HC0} =  {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^{\top}\mathrm{diag}\left[e_i^2\right]{\mathbf{X}}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
<p><strong>HC1</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{HC1} =  \frac{N}{N-K}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^{\top}\mathrm{diag}\left[e_i^2\right]{\mathbf{X}}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span> where <span class="math inline">\(N\)</span> is the number of observations and <span class="math inline">\(K\)</span> is the number of elements in <span class="math inline">\(\beta\)</span>.</p>
<p><strong>HC3</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{HC3} =  {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^{\top}\mathrm{diag}\left[\frac{e_i^2}{(1-h_{ii})^2}\right]{\mathbf{X}}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
</div>
<div id="cluster-robust-variance" class="section level4">
<h4 class="hasAnchor">
<a href="#cluster-robust-variance" class="anchor"></a>Cluster-Robust Variance</h4>
<p><strong>Definitions</strong></p>
<ul>
<li>
<span class="math inline">\(S\)</span> is the number of clusters</li>
<li>
<span class="math inline">\({\mathbf{X}}_s\)</span> is the rows of <span class="math inline">\({\mathbf{X}}\)</span> that belong to cluster <span class="math inline">\(s\)</span>
</li>
<li><span class="math inline">\({\mathbf{P}}_{ss} = {\mathbf{X}}_s {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^\top_s\)</span></li>
<li>
<span class="math inline">\(N_s\)</span> is the number of units in cluster <span class="math inline">\(s\)</span>
</li>
<li>
<span class="math inline">\(I_n\)</span> is an identity matrix of size <span class="math inline">\(n\times n\)</span>
</li>
<li>
<span class="math inline">\({\mathbf{e}}_s\)</span> is the elements of the residual matrix <span class="math inline">\({\mathbf{e}}\)</span> in cluster <span class="math inline">\(s\)</span>, or <span class="math inline">\({\mathbf{e}}_s = {\mathbf{y}}_s - {\mathbf{X}}_s \hat{\beta}\)</span>
</li>
</ul>
<p><strong>BM (default, also known as CR2 or Bell-McCaffrey)</strong></p>
<p>Analogy to HC2 for cluster-robust variance estimation. For the original reference, see <span class="citation">(Bell and McCaffrey <a href="#ref-bellmccaffrey2002">2002</a>)</span>. For papers that have also mention it, see <span class="math inline">\({\mathbb{V}}_{\mathrm{LZ2}}\)</span> on p. 709 of <span class="citation">(Imbens and Kolesar <a href="#ref-imbenskolesar2016">2016</a>)</span> and the CR2 estimator shown in equations (4) and (5) in <span class="citation">(Pustejovsky and Tipton <a href="#ref-pustejovskytipton2016">2016</a>)</span>. Note that we will be implementing the estimator proposed in <span class="citation">(Pustejovsky and Tipton <a href="#ref-pustejovskytipton2016">2016</a>)</span> soon; more can be found on their GitHub page for an accompanying R package <a href="https://github.com/jepusto/clubSandwich">clubSandwich</a>.</p>
<p><span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{BM} =  {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\sum^S_{s=1} \left[{\mathbf{X}}^\top_s (I_{N_s} - {\mathbf{P}}_{ss})^{-\frac{1}{2}} {\mathbf{e}}_s{\mathbf{e}}^\top_s (I_{N_s} - {\mathbf{P}}_{ss})^{-\frac{1}{2}} {\mathbf{X}}_s \right] {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
<p><strong>stata</strong></p>
<p><span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{stata} = \frac{N-1}{N-K}\frac{S}{S-1} {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\sum^S_{s=1} \left[{\mathbf{X}}^\top_s {\mathbf{e}}_s{\mathbf{e}}^\top_s {\mathbf{X}}_s \right] {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
</div>
<div id="classical" class="section level4">
<h4 class="hasAnchor">
<a href="#classical" class="anchor"></a>Classical</h4>
<p><strong>classical</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\beta}]_{classical} =  \frac{{\mathbf{e}}^\top{\mathbf{e}}}{N-K} {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}\]</span></p>
</div>
</div>
<div id="confidence-intervals-and-hypothesis-testing" class="section level3">
<h3 class="hasAnchor">
<a href="#confidence-intervals-and-hypothesis-testing" class="anchor"></a>Confidence intervals and hypothesis testing</h3>
<p>For confidence intervals and hypothesis testing, we use <span class="math inline">\(t\)</span>-statistics and thus require degrees of freedom. For all specifications that <em>do not involve clustering</em>,</p>
<p><span class="math display">\[
df = N - K
\]</span></p>
<p>When there is clustering and <code>se_type = "stata"</code> and, we match Stata’s conservative degrees of freedom by setting</p>
<p><span class="math display">\[
df_{stata} = S - 1
\]</span></p>
<p>When there is clustering and <code>se_type = "BM"</code>, we match Bell and McCaffrey’s degrees of freedom adjustment (this is described in detail on p. 709 of <span class="citation">(Imbens and Kolesar <a href="#ref-imbenskolesar2016">2016</a>)</span> as well as equation (11) in <span class="citation">(Pustejovsky and Tipton <a href="#ref-pustejovskytipton2016">2016</a>)</span>). Where <span class="math inline">\(z_{K,k}\)</span> is a vector of length <span class="math inline">\(K\)</span> where the <span class="math inline">\(k\)</span>th element is 1 and all other elements are 0. The <span class="math inline">\(k\)</span> signifies for which coefficient we are getting the degrees of freedom.</p>
<p>Also, <span class="math inline">\({\mathbf{P}}= {\mathbf{X}}{({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}{\mathbf{X}}^\top\)</span> and <span class="math inline">\((I - {\mathbf{P}})_s\)</span> is the <span class="math inline">\(N_s\)</span> columns that correspond to cluster <span class="math inline">\(s\)</span>.</p>
<p>We define <span class="math inline">\({\mathbf{G}}\)</span> as an <span class="math inline">\(N\times S\)</span> matrix with each column as</p>
<p><span class="math display">\[
{\mathbf{G}}_s = (I-{\mathbf{P}})_s (I_{N_s} - {\mathbf{P}}_{ss})^{-\frac{1}{2}} {\mathbf{X}}_s {({\mathbf{X}}^{\top}{\mathbf{X}})^{-1}}z_{K, k}
\]</span></p>
<p>Then the degrees of freedom for coefficient <span class="math inline">\(k\)</span> is</p>
<p><span class="math display">\[
df_{k, BM} = \frac{\left(\sum^S_{s=1} \lambda_i\right)^2}{\sum^S_{s=1} \lambda^2_i}
\]</span> where <span class="math inline">\(\lambda_i\)</span> are the eigenvalues of <span class="math inline">\({\mathbf{G}}^\top{\mathbf{G}}\)</span>.</p>
<p>Then, if <span class="math inline">\(\hat{{\mathbb{V}}}_k\)</span> is the <span class="math inline">\(k\)</span>th diagonal element of <span class="math inline">\(\hat{{\mathbb{V}}}\)</span>, we build confidence intervals using the user specified <span class="math inline">\(\alpha\)</span> as:</p>
<p><span class="math display">\[
\mathrm{CI}^{1-\alpha} = \left(\hat{\beta_k} + t^{df}_{\alpha/2} \sqrt{\hat{{\mathbb{V}}}_k}, \hat{\beta_k} + t^{df}_{1 - \alpha/2} \sqrt{\hat{{\mathbb{V}}}_k}\right)
\]</span></p>
<p>The associated p-values for a two-sided null hypothesis test where the null is that the coefficient is 0 uses a t-distribution with the aforementioned significance level <span class="math inline">\(\alpha\)</span> and degrees of freedom <span class="math inline">\(df\)</span>.</p>
</div>
</div>
<div id="lm_lin" class="section level2">
<h2 class="hasAnchor">
<a href="#lm_lin" class="anchor"></a><code><a href="../reference/lm_lin.html">lm_lin()</a></code>
</h2>
<p>The <code><a href="../reference/lm_lin.html">lm_lin()</a></code> estimator is a data pre-processor for <code><a href="../reference/lm_robust.html">lm_robust()</a></code> that implements the regression method for covariate adjustment suggested by <span class="citation">(Lin and others <a href="#ref-lin2013">2013</a>)</span>. In response to the critique by <span class="citation">(Freedman <a href="#ref-freedman2008">2008</a>)</span> that using regression to adjust for pre-treatment covariates could bias estimates of treatment effects, <span class="citation">(Lin and others <a href="#ref-lin2013">2013</a>)</span> demonstrates the small magnitude of this bias while also presenting an estimator to reduce said bias.</p>
<p>This estimator takes the outcome and treatment variable as the main formula (<code>formula</code>) and takes a right-sided formula of all pre-treatment covariates (<code>covariates</code>). These pre-treatment covariates are then centered to be mean zero and interacted with the treatment variable before being added to the formula and passed to <code><a href="../reference/lm_robust.html">lm_robust()</a></code>.</p>
<p>The rest of the estimation proceeds just as in <a href="#lm_robust"><code><a href="../reference/lm_robust.html">lm_robust()</a></code></a>.</p>
</div>
<div id="difference_in_means" class="section level2">
<h2 class="hasAnchor">
<a href="#difference_in_means" class="anchor"></a><code><a href="../reference/difference_in_means.html">difference_in_means()</a></code>
</h2>
<p>There are six kinds of experimental designs for which our <code><a href="../reference/difference_in_means.html">difference_in_means()</a></code> estimator can estimate treatment effects, provide estiamtes of uncertainty, construct confidence intervals, and provide p-values. We list them here along with how the software learns the design:</p>
<ul>
<li>Simple (both <code>cluster_variable_name</code> and <code>block_variable_name</code> are unused)</li>
<li>Clustered (<code>cluster_variable_name</code> is specified while <code>block_variable_name</code> is not)</li>
<li>Blocked (<code>block_variable_name</code> is specified while <code>cluster_variable_name</code> is not)</li>
<li>Blocked and clustered (both are specified)</li>
</ul>
<p>There are two subsets of blocked designs that we also consider:</p>
<ul>
<li>Matched-pairs (only <code>block_variable_name</code> is specified and all blocks are size two)</li>
<li>Matched-pair clustered design (both names are specified and each block only has two clusters)</li>
</ul>
<p>In addition, weights can be specified.</p>
<div id="estimates" class="section level3">
<h3 class="hasAnchor">
<a href="#estimates" class="anchor"></a>Estimates</h3>
<p><strong>Any unblocked design</strong> <span class="math display">\[
\hat{\tau} = \frac{1}{N} \sum^N_{i=1} z_i y_i - (1 - z_i) y_i
\]</span> where <span class="math inline">\(z_i\)</span> is the treatment variable, <span class="math inline">\(y_i\)</span> is the outcome, and <span class="math inline">\(N\)</span> is the total number of units.</p>
<p><strong>Blocked design (including matched-pairs designs)</strong> <span class="math display">\[
\hat{\tau} = \sum^J_{j=1} \frac{N_j}{N} \hat{\tau_j}
\]</span> where <span class="math inline">\(J\)</span> is the number of blocks, <span class="math inline">\(N_j\)</span> is the size of those blocks, and <span class="math inline">\(\hat{\tau_j}\)</span> is the estimated difference-in-means in block <span class="math inline">\(j\)</span>.</p>
</div>
<div id="variance-1" class="section level3">
<h3 class="hasAnchor">
<a href="#variance-1" class="anchor"></a>Variance</h3>
<p><strong>Simple</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\tau}] = \frac{\hat{{\mathbb{V}}}[y_{i,0}]}{N_0} + \frac{\hat{{\mathbb{V}}}[y_{i,1}]}{N_1}
\]</span> where <span class="math inline">\(\hat{{\mathbb{V}}}[y_{i,k}]\)</span> is the Bessel-corrected variance of all units where <span class="math inline">\(z_i = k\)</span> and <span class="math inline">\(N_k\)</span> is the number of units in condition <span class="math inline">\(k\)</span>.</p>
<p><strong>Clustered</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\tau}] = \frac{N\hat{{\mathbb{V}}}[y_{i,0}]}{SN_0} + \frac{N\hat{{\mathbb{V}}}[y_{i,1}]}{SN_1}
\]</span> where <span class="math inline">\(S\)</span> is the number of clusters. See equation 3.23 on page 83 of <span class="citation">(Gerber and Green <a href="#ref-gerbergreen2012">2012</a>)</span> for a reference.</p>
<p><strong>Blocked &amp; blocked and clustered</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\tau}] = \sum^J_{j=1} \left(\frac{N_j}{N}\right)^2 \hat{{\mathbb{V}}}[\hat{\tau_j}] 
\]</span> where <span class="math inline">\(\hat{{\mathbb{V}}}[\hat{\tau_j}]\)</span> is the variance of the estimated difference-in-means in block <span class="math inline">\(j\)</span>. See footnote 17 on page 74 of <span class="citation">(Gerber and Green <a href="#ref-gerbergreen2012">2012</a>)</span> for a reference.</p>
<p><strong>Matched-pairs</strong> <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\tau}] = \frac{1}{J(J-1)} \sum^J_{j=1} \left(\hat{\tau_j} - \hat{\tau}\right)^2 
\]</span> See equation 3.16 on page 77 of <span class="citation">(Gerber and Green <a href="#ref-gerbergreen2012">2012</a>)</span> for a reference.</p>
<p><strong>Matched-pair cluster randomized</strong> This formula is the variance of the SATE defined in Equation 6 on page 36 of <span class="citation">(Imai et al. <a href="#ref-imaietal2009">2009</a>)</span>. <span class="math display">\[
\hat{{\mathbb{V}}}[\hat{\tau}] = \frac{J}{(J-1)N^2} \sum^J_{j=1} \left(N_j \hat{\tau_j} - \frac{N \hat{\tau}}{J}\right)^2
\]</span></p>
</div>
<div id="confidence-intervals-and-hypothesis-testing-1" class="section level3">
<h3 class="hasAnchor">
<a href="#confidence-intervals-and-hypothesis-testing-1" class="anchor"></a>Confidence intervals and hypothesis testing</h3>
<p>In order to estimate confidence intervals and produce p-values, we first must estimate the degrees of freedom.</p>
<p><strong>Simple</strong> We use the well known Welch-Satterthwaite equation to estimate the degrees of freedom for simple designs. <span class="math display">\[
df = \frac{(\hat{{\mathbb{V}}}[\hat{\tau}])^2}{\frac{(\hat{{\mathbb{V}}}[y_{i,0}])^2}{N^2_0(N_0-1)} + \frac{(\hat{{\mathbb{V}}}[y_{i,1}])^2}{N^2_1(N_1-1)}}
\]</span></p>
<p><strong>Clustered</strong> We use the same degrees of freedom above for cluster randomized experiments. However, <span class="citation">(Gerber and Green <a href="#ref-gerbergreen2012">2012</a>)</span> recommend in footnote 20 on page 83 that the bounds of the confidence interval must be expanded by <span class="math inline">\(\sqrt{(J-1)/(J-2)}\)</span>. We do not implement this. TODO: instead of expanding confidence intervals, find equivalent degrees of freedom correction so that p-values and hypothesis tests are in accordance.</p>
<p><strong>Blocked &amp; block-clustered &amp; matched-pair</strong> TODO: check best practices on degrees of freedom with blocks. Here we set <span class="math display">\[
df = N - J - 1
\]</span> which matches the degrees of freedom when using regression to estimate treatment effects with dummy variables for each block.</p>
<p><strong>Matched-pair cluster randomized</strong> Following advice of <span class="citation">(Imai et al. <a href="#ref-imaietal2009">2009</a>)</span> on page 37, we use the following conservative estimate of the degrees of freedom: <span class="math display">\[
df_{MPCR} = J - 1
\]</span></p>
</div>
</div>
<div id="horvitz_thompson" class="section level2">
<h2 class="hasAnchor">
<a href="#horvitz_thompson" class="anchor"></a><code><a href="../reference/horvitz_thompson.html">horvitz_thompson()</a></code>
</h2>
<p>TODO</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-bellmccaffrey2002">
<p>Bell, Robert M, and Daniel F McCaffrey. 2002. “Bias Reduction in Standard Errors for Linear Regression with Multi-Stage Samples.” <em>Survey Methodology</em> 28 (2): 169–82.</p>
</div>
<div id="ref-freedman2008">
<p>Freedman, David A. 2008. “On Regression Adjustments in Experiments with Several Treatments.” <em>The Annals of Applied Statistics</em>. JSTOR, 176–96. <a href="http://dx.doi.org/10.1214/07-AOAS143" class="uri">http://dx.doi.org/10.1214/07-AOAS143</a>.</p>
</div>
<div id="ref-gerbergreen2012">
<p>Gerber, Alan S, and Donald P Green. 2012. <em>Field Experiments: Design, Analysis, and Interpretation</em>. New York: W.W. Norton.</p>
</div>
<div id="ref-imaietal2009">
<p>Imai, Kosuke, Gary King, Clayton Nall, and others. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” <em>Statistical Science</em> 24 (1). Institute of Mathematical Statistics: 29–53. <a href="http://dx.doi.org/10.1214/08-STS274" class="uri">http://dx.doi.org/10.1214/08-STS274</a>.</p>
</div>
<div id="ref-imbenskolesar2016">
<p>Imbens, Guido W, and Michal Kolesar. 2016. “Robust Standard Errors in Small Samples: Some Practical Advice.” <em>Review of Economics and Statistics</em> 98 (4). MIT Press: 701–12. <a href="http://dx.doi.org/10.1162/REST_a_00552" class="uri">http://dx.doi.org/10.1162/REST_a_00552</a>.</p>
</div>
<div id="ref-lin2013">
<p>Lin, Winston, and others. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” <em>The Annals of Applied Statistics</em> 7 (1). Institute of Mathematical Statistics: 295–318. <a href="http://dx.doi.org/10.1214/12-AOAS583" class="uri">http://dx.doi.org/10.1214/12-AOAS583</a>.</p>
</div>
<div id="ref-pustejovskytipton2016">
<p>Pustejovsky, James E, and Elizabeth Tipton. 2016. “Small Sample Methods for Cluster-Robust Variance Estimation and Hypothesis Testing in Fixed Effects Models.” <em>Journal of Business &amp; Economic Statistics</em>. Taylor &amp; Francis. <a href="http://dx.doi.org/10.1080/07350015.2016.1247004" class="uri">http://dx.doi.org/10.1080/07350015.2016.1247004</a>.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#hypothetical-experiment">Hypothetical experiment</a></li>
      <li>
<a href="#estimators">Estimators</a><ul class="nav nav-pills nav-stacked">
<li><a href="#lm_robust"><code><a href="../reference/lm_robust.html">lm_robust()</a></code></a></li>
      <li><a href="#lm_lin"><code><a href="../reference/lm_lin.html">lm_lin()</a></code></a></li>
      <li><a href="#difference_in_means"><code><a href="../reference/difference_in_means.html">difference_in_means()</a></code></a></li>
      <li><a href="#horvitz_thompson"><code><a href="../reference/horvitz_thompson.html">horvitz_thompson()</a></code></a></li>
      </ul>
</li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Graeme Blair, Jasper Cooper, Alexander Coppock, Macartan Humphreys.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
